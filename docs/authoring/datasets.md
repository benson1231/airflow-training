# Datasets in Airflow

**2025.10.22**

## ğŸ§© æ¦‚å¿µèªªæ˜

`Dataset` æ˜¯ Airflow 2.4 å¼•å…¥çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œç”¨æ–¼åœ¨**ä¸åŒ DAG ä¹‹é–“**å»ºç«‹ã€Œè³‡æ–™ç›¸ä¾æ€§ (data dependencies)ã€ã€‚èˆ‡å‚³çµ±çš„ä»»å‹™å±¤ç´šä¾è³´ (`task dependencies`) ä¸åŒï¼ŒDatasets è®“ä½ èƒ½ä»¥**è³‡æ–™æ›´æ–°äº‹ä»¶**ä¾†è§¸ç™¼ä¸‹æ¸¸æµç¨‹ï¼Œè€Œéåƒ…ä¾æ“šæ’ç¨‹æ™‚é–“ã€‚

æ›å¥è©±èªªï¼ŒDataset è®“ Airflow å¾ã€Œæ™‚é–“å°å‘ (time-based scheduling)ã€é€²åŒ–ç‚ºã€Œè³‡æ–™å°å‘ (data-driven scheduling)ã€ã€‚

---

## ğŸ§  åŸºæœ¬æ¦‚å¿µ

* **Dataset å®šç¾©**ï¼šä½¿ç”¨ `Dataset(uri)` ç‰©ä»¶å®šç¾©ä¸€å€‹è³‡æ–™é›†ï¼Œé€šå¸¸ä»£è¡¨æª”æ¡ˆã€è¡¨æ ¼ã€è³‡æ–™åº«æˆ–ä»»æ„å¯è¾¨è­˜çš„è³‡æºã€‚
* **Producer DAG**ï¼šè² è²¬æ›´æ–°æˆ–ç”¢ç”Ÿ Dataset çš„ DAGã€‚
* **Consumer DAG**ï¼šç•¶ Dataset è¢«æ›´æ–°æ™‚è‡ªå‹•è§¸ç™¼çš„ DAGã€‚

---

## ğŸ§± å®šç¾© Dataset

```python
from airflow import Dataset

# å®šç¾©ä¸€å€‹è³‡æ–™é›†ï¼Œä¾‹å¦‚å„²å­˜åœ¨ S3 æˆ–æœ¬åœ°çš„ CSV æª”æ¡ˆ
my_dataset = Dataset("s3://data/processed/customers.csv")
```

---

## ğŸš€ å»ºç«‹ Producer èˆ‡ Consumer

```python
from airflow import DAG
from airflow.decorators import task
from datetime import datetime

# (1) Producer DAG: æ›´æ–° dataset
@dag(schedule="@daily", start_date=datetime(2024,1,1), catchup=False)
def producer_dag():
    @task(outlets=[my_dataset])
    def generate_data():
        print("Writing customers.csv ...")

    generate_data()

# (2) Consumer DAG: ç›£è½ dataset æ›´æ–°äº‹ä»¶
@dag(schedule=[my_dataset], start_date=datetime(2024,1,1), catchup=False)
def consumer_dag():
    @task
    def analyze_data():
        print("Triggered because customers.csv was updated!")

    analyze_data()
```

æ­¤ç¯„ä¾‹ä¸­ï¼Œç•¶ `producer_dag` æ›´æ–° `customers.csv` æ™‚ï¼ŒAirflow æœƒè‡ªå‹•è§¸ç™¼ `consumer_dag`ã€‚

---

## ğŸ§© å¤šé‡ Dataset ç›£è½

å¯è®“ä¸‹æ¸¸ DAG ç›£è½å¤šå€‹ Datasetï¼Œåªè¦ä»»ä¸€è³‡æ–™é›†è¢«æ›´æ–°å³è§¸ç™¼ï¼š

```python
from airflow import Dataset

customers = Dataset("s3://data/customers.csv")
orders = Dataset("s3://data/orders.csv")

@dag(schedule=[customers, orders], start_date=datetime(2024,1,1), catchup=False)
def downstream_dag():
    ...
```

---

## ğŸ§­ å¯¦å‹™å»ºè­°

1. **URI è¦ç¯„åŒ–**ï¼šå»ºè­°ä½¿ç”¨æ¨™æº–åŒ–è·¯å¾‘ï¼ˆå¦‚ `s3://`ã€`db://`ã€`file://`ï¼‰ï¼Œæ–¹ä¾¿æ—¥å¾Œå¯è¦–åŒ–èˆ‡æŸ¥è©¢ã€‚
2. **è³‡æ–™å¯è¦–åŒ–**ï¼šåœ¨ Airflow UIã€ŒDatasetsã€é ç±¤å¯æª¢è¦– Dataset é—œè¯åœ–ï¼ŒåŒ…å«ä¸Šæ¸¸/ä¸‹æ¸¸ DAGã€‚
3. **å‘½åä¸€è‡´æ€§**ï¼šä½¿ç”¨æ¨¡çµ„åŒ–å‘½åï¼ˆå¦‚ `Dataset('warehouse.sales_data')`ï¼‰æ–¹ä¾¿ç¶­è­·ã€‚
4. **æ··åˆæ¨¡å¼**ï¼šå¯åŒæ™‚ä¿ç•™æ™‚é–“æ’ç¨‹èˆ‡ Dataset è§¸ç™¼ï¼Œä¾‹å¦‚ï¼š

```python
@dag(schedule=[Dataset('s3://data/orders.csv'), '@weekly'])
```

---

## âš™ï¸ é™åˆ¶èˆ‡æ³¨æ„äº‹é …

* Dataset åƒ…æ”¯æ´ DAG å±¤ç´šè§¸ç™¼ï¼Œç„¡æ³•ç›´æ¥ç¶å®šè‡³å–®ä¸€ä»»å‹™ã€‚
* ä¸æ”¯æ´è·¨ç’°å¢ƒï¼ˆä¸åŒ Airflow instanceï¼‰çš„è³‡æ–™è§¸ç™¼ã€‚
* è‹¥ä¸Šæ¸¸ DAG å¤±æ•—æˆ–è·³éä»»å‹™ï¼ŒDataset ä¸æœƒè¢«æ›´æ–°ã€‚

---

## âœ… å°çµ

| æ¦‚å¿µ               | èªªæ˜                   |
| ---------------- | -------------------- |
| **Dataset**      | ä»£è¡¨ä¸€å€‹è³‡æ–™è³‡æºï¼ˆæª”æ¡ˆã€è¡¨æ ¼ã€è³‡æ–™åº«ç­‰ï¼‰ |
| **Producer DAG** | è² è²¬æ›´æ–° Dataset         |
| **Consumer DAG** | è¢« Dataset æ›´æ–°äº‹ä»¶è§¸ç™¼     |
| **ç”¨é€”**           | å»ºç«‹ DAG é–“çš„è³‡æ–™é©…å‹•ç›¸ä¾æ€§     |

---

## ğŸ“˜ å»¶ä¼¸é–±è®€

* [Airflow Datasets å®˜æ–¹æ–‡ä»¶](https://airflow.apache.org/docs/apache-airflow/stable/concepts/datasets.html)
* [Airflow Summit 2023: Data-Aware Scheduling ä»‹ç´¹å½±ç‰‡](https://www.youtube.com/watch?v=UDRQxnpdK1E)
